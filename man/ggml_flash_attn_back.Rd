% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/operations.R
\name{ggml_flash_attn_back}
\alias{ggml_flash_attn_back}
\title{Flash Attention Backward (Graph)}
\usage{
ggml_flash_attn_back(ctx, q, k, v, d, masked = TRUE)
}
\arguments{
\item{ctx}{GGML context}

\item{q}{Query tensor (same as forward pass)}

\item{k}{Key tensor (same as forward pass)}

\item{v}{Value tensor (same as forward pass)}

\item{d}{Gradient tensor from upstream (same shape as forward output)}

\item{masked}{Logical: whether causal masking was used in forward pass}
}
\value{
Gradient tensor
}
\description{
Backward pass for Flash Attention.
Used during training to compute gradients through attention.
}
