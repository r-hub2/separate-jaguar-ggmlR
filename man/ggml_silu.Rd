% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/operations.R
\name{ggml_silu}
\alias{ggml_silu}
\title{SiLU Activation (Graph)}
\usage{
ggml_silu(ctx, a)
}
\arguments{
\item{ctx}{GGML context}

\item{a}{Input tensor}
}
\value{
Tensor representing the SiLU operation
}
\description{
Creates a graph node for SiLU (Sigmoid Linear Unit) activation, also known as Swish.
CRITICAL for LLaMA models.
}
\examples{
\dontrun{
ctx <- ggml_init(1024 * 1024)
a <- ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 5)
ggml_set_f32(a, c(-2, -1, 0, 1, 2))
result <- ggml_silu(ctx, a)
graph <- ggml_build_forward_expand(ctx, result)
ggml_graph_compute(ctx, graph)
ggml_get_f32(result)
ggml_free(ctx)
}
}
