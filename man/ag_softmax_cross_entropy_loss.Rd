% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/autograd.R
\name{ag_softmax_cross_entropy_loss}
\alias{ag_softmax_cross_entropy_loss}
\title{Fused softmax + cross-entropy loss (numerically stable)}
\usage{
ag_softmax_cross_entropy_loss(logits, target)
}
\arguments{
\item{logits}{ag_tensor [classes, batch_size]  raw (pre-softmax) scores}

\item{target}{matrix [classes, batch_size]     one-hot labels}
}
\value{
scalar ag_tensor
}
\description{
Combines softmax and CE in one op using the fused gradient \code{(p - y) / n}.
More numerically stable than chaining \code{ag_softmax} + \code{ag_cross_entropy_loss}.
Use this when your last layer outputs raw logits.
}
