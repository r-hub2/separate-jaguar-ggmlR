---
title: "Keras-like Neural Networks with ggmlR"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Keras-like Neural Networks with ggmlR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

## Introduction

If you have used Keras in Python, ggmlR will feel immediately familiar. It
provides the same two model-building APIs — Sequential and Functional — while
removing every Python/TensorFlow dependency. ggmlR is a native R package backed
by the [ggml](https://github.com/ggml-org/ggml) C library, which supports both
CPU computation and GPU acceleration through the Vulkan backend. You get the
Keras mental model at a fraction of the install weight, with results that live
entirely inside your R session.

## 1. Sequential API — the classic entry point

The Sequential API is the fastest way to get a network running. Layers are
stacked with the pipe operator, compiled, and then trained with `ggml_fit()`.

```{r sequential}
library(ggmlR)

# Build a simple MLP classifier
model <- ggml_model_sequential() |>
  ggml_layer_dense(128L, activation = "relu", input_shape = 64L) |>
  ggml_layer_dropout(0.3) |>
  ggml_layer_dense(10L, activation = "softmax")

# Compile: choose optimizer and loss
model <- ggml_compile(model,
                      optimizer = "adam",
                      loss      = "categorical_crossentropy")

# Fit on (x, y) — x is a matrix [n_samples × 64], y is [n_samples × 10]
model <- ggml_fit(model, x, y,
                  epochs     = 5L,
                  batch_size = 32L,
                  verbose    = 0L)

# Predict on new data
preds <- ggml_predict(model, x)
```

> **Note on `input_shape`:** unlike Keras, the batch dimension is *not* part of
> `input_shape`. Pass only the feature dimensions (here `64L` = number of
> features per sample).

## 2. Functional API — residual (skip) connection

The Functional API lets you wire layers into arbitrary directed acyclic graphs.
The most common use-case is a residual (skip) connection, which is impossible
to express in the Sequential API.

```{r functional-residual}
library(ggmlR)

inp <- ggml_input(shape = 64L)

# Project and apply non-linearity
x   <- inp |> ggml_layer_dense(64L, activation = "relu")

# Skip connection: add the original input to the transformed output
res <- ggml_layer_add(list(inp, x))

# Classification head
out <- res |> ggml_layer_dense(10L, activation = "softmax")

# Assemble and compile
m <- ggml_model(inputs = inp, outputs = out)
m <- ggml_compile(m,
                  optimizer = "adam",
                  loss      = "categorical_crossentropy")
```

The key difference from Keras: merge layers take an explicit R `list()` rather
than a Python list, e.g. `ggml_layer_add(list(a, b))`.

## 3. Embedding + NLP (integer token input)

For NLP tasks the input is integer token indices. Declare the input with
`dtype = "int32"`, attach an embedding layer, then continue as usual.

```{r embedding}
library(ggmlR)

# Input: integer token sequences of length 20
inp <- ggml_input(shape = 20L, dtype = "int32")

out <- inp |>
  ggml_layer_embedding(vocab_size = 1000L, dim = 64L) |>
  ggml_layer_flatten() |>
  ggml_layer_dense(10L, activation = "softmax")

m <- ggml_model(inputs = inp, outputs = out)
```

> Token values must be 0-based integers in the range `[0, vocab_size - 1]`.
> The input matrix fed to `ggml_fit()` / `ggml_predict()` should have shape
> `[n_samples × seq_len]` with storage mode `integer`.

## 4. Multi-output: feature extraction

Pass a `list()` of tensors as `outputs` to obtain intermediate activations
alongside the final predictions. This is the ggmlR equivalent of Keras
feature-extraction models.

```{r multi-output}
library(ggmlR)

inp    <- ggml_input(shape = 64L)
hidden <- inp    |> ggml_layer_dense(64L, activation = "relu")
out    <- hidden |> ggml_layer_dense(10L, activation = "softmax")

# Expose both the hidden layer and the classifier output
m <- ggml_model(inputs = inp, outputs = list(hidden, out))

preds <- ggml_predict(m, x)
# preds[[1]] — hidden-layer activations, shape [n_samples × 64]
# preds[[2]] — class probabilities,      shape [n_samples × 10]
```

## 5. Shared layers (Siamese / weight sharing)

In a Siamese network the same encoder is applied to two different inputs, and
its weights are shared. In ggmlR you first create a *layer object* with
`ggml_dense()` (or other `ggml_*` layer constructors), then apply it to
different inputs with `ggml_apply()`.

```{r siamese}
library(ggmlR)

# Define the shared encoder once
enc <- ggml_dense(32L, activation = "relu", name = "encoder")

# Two independent inputs
x1 <- ggml_input(shape = 16L, name = "left")
x2 <- ggml_input(shape = 16L, name = "right")

# Apply the *same* layer (identical weights) to both branches
h1 <- ggml_apply(x1, enc)
h2 <- ggml_apply(x2, enc)

# Merge and classify
out <- ggml_layer_add(list(h1, h2)) |>
  ggml_layer_dense(2L, activation = "softmax")

m <- ggml_model(inputs = list(x1, x2), outputs = out)
```

## Differences from Keras

| Feature | Keras (Python) | ggmlR |
|---|---|---|
| Axis indexing | 0-based | 0-based (same) |
| Batch dimension | explicit in `input_shape` | implicit (excluded from shape) |
| Merge layers | `add([a, b])` | `ggml_layer_add(list(a, b))` |
| Shared layers | reuse layer object | `ggml_dense()` + `ggml_apply()` |
| Multi-output predict | numpy array list | R list of matrices |
| Sequential create | `keras_model_sequential()` | `ggml_model_sequential()` |
| Backend | TensorFlow / JAX / PyTorch | ggml (CPU + Vulkan GPU) |
